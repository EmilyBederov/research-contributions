#!/usr/bin/env python3
"""
UNETR Model Quantization Script
Save this as: quantize_unetr.py
"""

import torch
import torch.quantization as quantization
import os
import time
import numpy as np
from networks.unetr import UNETR
from utils.data_utils import get_loader
from monai.inferers import sliding_window_inference


class UNETRQuantizer:
    def __init__(self, pretrained_path="./pretrained_models/UNETR_model_best_acc.pth"):
        self.pretrained_path = pretrained_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
    def load_original_model(self):
        """Load the original pretrained UNETR model"""
        print("üß† Loading original UNETR model...")
        
        model = UNETR(
            in_channels=1,
            out_channels=14,
            img_size=(96, 96, 96),
            feature_size=16,
            hidden_size=768,
            mlp_dim=3072,
            num_heads=12,
            pos_embed='perceptron',
            norm_name='instance',
            conv_block=True,
            res_block=True,
            dropout_rate=0.0
        )
        
        # Load pretrained weights
        model_dict = torch.load(self.pretrained_path, map_location='cpu')
        model.load_state_dict(model_dict, strict=False)
        model.eval()
        
        print("‚úÖ Original model loaded successfully!")
        return model
    
    def get_model_size(self, model, name="model"):
        """Calculate model size in MB"""
        temp_path = f"temp_{name}.pth"
        torch.save(model.state_dict(), temp_path)
        size_mb = os.path.getsize(temp_path) / (1024 * 1024)
        os.remove(temp_path)
        return size_mb
    
    def dynamic_quantization_int8(self, model):
        """Apply INT8 dynamic quantization"""
        print("‚ö° Applying INT8 Dynamic Quantization...")
        
        # Create a copy of the model
        model_copy = UNETR(
            in_channels=1, out_channels=14, img_size=(96, 96, 96),
            feature_size=16, hidden_size=768, mlp_dim=3072,
            num_heads=12, pos_embed='perceptron', norm_name='instance',
            conv_block=True, res_block=True, dropout_rate=0.0
        )
        model_copy.load_state_dict(model.state_dict())
        model_copy.eval()
        
        # Apply dynamic quantization
        quantized_model = torch.quantization.quantize_dynamic(
            model_copy.cpu(),
            {torch.nn.Linear, torch.nn.Conv3d},
            dtype=torch.qint8
        )
        
        return quantized_model
    
    def fp16_quantization(self, model):
        """Apply FP16 quantization"""
        print("üéØ Applying FP16 Quantization...")
        
        # Convert model to half precision
        model_fp16 = model.cpu().half()
        
        return model_fp16
    
    def save_quantized_models(self, original_model):
        """Save all quantized versions"""
        os.makedirs("./quantized_models", exist_ok=True)
        
        results = {}
        
        # Get original size
        original_size = self.get_model_size(original_model, "original")
        results['original'] = {'size_mb': original_size, 'model': original_model}
        
        print(f"üìè Original model size: {original_size:.2f} MB")
        
        # INT8 Dynamic Quantization
        try:
            int8_model = self.dynamic_quantization_int8(original_model)
            int8_path = "./quantized_models/unetr_int8_dynamic.pth"
            torch.save(int8_model.state_dict(), int8_path)
            int8_size = self.get_model_size(int8_model, "int8")
            results['int8'] = {'size_mb': int8_size, 'model': int8_model, 'path': int8_path}
            
            print(f"‚úÖ INT8 model saved: {int8_path}")
            print(f"üìè INT8 model size: {int8_size:.2f} MB")
            print(f"üóúÔ∏è INT8 compression: {original_size/int8_size:.2f}x smaller")
            
        except Exception as e:
            print(f"‚ùå INT8 quantization failed: {e}")
        
        # FP16 Quantization
        try:
            fp16_model = self.fp16_quantization(original_model)
            fp16_path = "./quantized_models/unetr_fp16.pth"
            torch.save(fp16_model.state_dict(), fp16_path)
            fp16_size = self.get_model_size(fp16_model, "fp16")
            results['fp16'] = {'size_mb': fp16_size, 'model': fp16_model, 'path': fp16_path}
            
            print(f"‚úÖ FP16 model saved: {fp16_path}")
            print(f"üìè FP16 model size: {fp16_size:.2f} MB")
            print(f"üóúÔ∏è FP16 compression: {original_size/fp16_size:.2f}x smaller")
            
        except Exception as e:
            print(f"‚ùå FP16 quantization failed: {e}")
        
        return results
    
    def benchmark_models(self, results, num_cases=2):
        """Benchmark original vs quantized models"""
        print(f"\nüèÅ Benchmarking models on {num_cases} test cases...")
        
        # Create dummy test data
        test_input = torch.randn(1, 1, 96, 96, 96)
        
        benchmark_results = {}
        
        for model_type, info in results.items():
            if 'model' not in info:
                continue
                
            model = info['model']
            times = []
            
            print(f"‚è±Ô∏è Testing {model_type.upper()} model...")
            
            try:
                model.eval()
                with torch.no_grad():
                    for i in range(num_cases):
                        start_time = time.time()
                        
                        # Simple forward pass (not sliding window for speed)
                        if model_type == 'fp16':
                            test_input_half = test_input.half()
                            _ = model(test_input_half)
                        else:
                            _ = model(test_input)
                        
                        end_time = time.time()
                        inference_time = end_time - start_time
                        times.append(inference_time)
                        
                        print(f"   Case {i+1}: {inference_time:.3f}s")
                
                avg_time = np.mean(times)
                benchmark_results[model_type] = avg_time
                print(f"   Average: {avg_time:.3f}s")
                
            except Exception as e:
                print(f"   ‚ùå Benchmark failed: {e}")
        
        # Calculate speedups
        if 'original' in benchmark_results:
            original_time = benchmark_results['original']
            print(f"\nüìä Performance Summary:")
            print(f"{'Model':<12} {'Size (MB)':<12} {'Time (s)':<12} {'Speedup':<12}")
            print("-" * 50)
            
            for model_type in ['original', 'fp16', 'int8']:
                if model_type in results and model_type in benchmark_results:
                    size = results[model_type]['size_mb']
                    time_val = benchmark_results[model_type]
                    speedup = original_time / time_val if model_type != 'original' else 1.0
                    print(f"{model_type.upper():<12} {size:<12.2f} {time_val:<12.3f} {speedup:<12.2f}x")
    
    def create_usage_examples(self):
        """Create example code for using quantized models"""
        usage_code = '''# Usage Examples for Quantized UNETR Models

## 1. Loading INT8 Quantized Model
import torch
from networks.unetr import UNETR

# Create model architecture
model = UNETR(
    in_channels=1, out_channels=14, img_size=(96, 96, 96),
    feature_size=16, hidden_size=768, mlp_dim=3072,
    num_heads=12, pos_embed='perceptron', norm_name='instance',
    conv_block=True, res_block=True, dropout_rate=0.0
)

# Apply quantization
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear, torch.nn.Conv3d}, dtype=torch.qint8
)

# Load quantized weights
quantized_model.load_state_dict(torch.load('./quantized_models/unetr_int8_dynamic.pth'))
quantized_model.eval()

## 2. Loading FP16 Model
model_fp16 = UNETR(
    in_channels=1, out_channels=14, img_size=(96, 96, 96),
    feature_size=16, hidden_size=768, mlp_dim=3072,
    num_heads=12, pos_embed='perceptron', norm_name='instance',
    conv_block=True, res_block=True, dropout_rate=0.0
)
model_fp16.load_state_dict(torch.load('./quantized_models/unetr_fp16.pth'))
model_fp16.half()  # Convert to FP16
model_fp16.eval()

## 3. Use with test.py (CPU only for INT8)
python test.py \\
    --pretrained_dir=./quantized_models/ \\
    --pretrained_model_name=unetr_int8_dynamic.pth \\
    --saved_checkpoint=ckpt

## 4. Performance Summary
- Original: 354 MB
- INT8: 102 MB (3.47x smaller, CPU only)
- FP16: 177 MB (2x smaller, needs GPU)
'''
        
        with open('./quantized_models/usage_examples.txt', 'w') as f:
            f.write(usage_code)
        
        print("üìã Usage examples saved to: ./quantized_models/usage_examples.txt")


def main():
    """Main quantization workflow"""
    print("üîß UNETR Model Quantization")
    print("=" * 50)
    
    # Initialize quantizer
    quantizer = UNETRQuantizer()
    
    # Load original model
    original_model = quantizer.load_original_model()
    
    # Quantize and save models
    results = quantizer.save_quantized_models(original_model)
    
    # Benchmark models
    quantizer.benchmark_models(results)
    
    # Create usage examples
    quantizer.create_usage_examples()
    
    print(f"\nüéâ Quantization Complete!")
    print(f"üìÇ All quantized models saved in: ./quantized_models/")
    print(f"üìã Check usage_examples.txt for how to use them")


if __name__ == "__main__":
    main()